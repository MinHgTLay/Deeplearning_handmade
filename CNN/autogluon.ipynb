{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # 检查 PyTorch 版本\n",
    "print(torch.version.cuda)  # 检查 CUDA 版本\n",
    "print(torch.cuda.is_available())  # 检查 CUDA 是否可用\n",
    "print(torch.cuda.device_count())  # Should be > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "#@save\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件，返回本地文件名\"\"\"\n",
    "    assert name in DATA_HUB, f\"{name} 不存在于 {DATA_HUB}\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    print(f'正在从{url}下载{fname}...')\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "def download_extract(name, folder=None):  #@save\n",
    "    \"\"\"下载并解压zip/tar文件\"\"\"\n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        fp = zipfile.ZipFile(fname, 'r')\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "    else:\n",
    "        assert False, '只有zip/tar文件可以被解压缩'\n",
    "    fp.extractall(base_dir)\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir\n",
    "\n",
    "def download_all():  #@save\n",
    "    \"\"\"下载DATA_HUB中的所有文件\"\"\"\n",
    "    for name in DATA_HUB:\n",
    "        download(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB['kaggle_house_train'] = (  #@save\n",
    "    DATA_URL + 'kaggle_house_pred_train.csv',\n",
    "    '585e9cc93e70b39160e7921475f9bcd7d31219ce')\n",
    "\n",
    "DATA_HUB['kaggle_house_test'] = (  #@save\n",
    "    DATA_URL + 'kaggle_house_pred_test.csv',\n",
    "    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "train_data = pd.read_csv(download('kaggle_house_train'))\n",
    "test_data = pd.read_csv(download('kaggle_house_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: C:\\DeepLearning\\california-house-prices\\train.csv | Columns = 41 / 41 | Rows = 47439 -> 47439\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250130_022418\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          24\n",
      "Memory Avail:       16.29 GB / 31.12 GB (52.4%)\n",
      "Disk Space Avail:   678.21 GB / 924.66 GB (73.3%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (47439 samples, 107.1 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"c:\\DeepLearning\\AutogluonModels\\ag-20250130_022418\"\n",
      "Train Data Rows:    47439\n",
      "Train Data Columns: 39\n",
      "Label Column:       Sold Price\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (90000000.0, 100500.0, 1296050.49915, 1694452.20335)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16757.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 101.78 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id    Sold Price    Year built           Lot     Bathrooms  \\\n",
      "count  47439.000000  4.743900e+04  46394.000000  3.325800e+04  43974.000000   \n",
      "mean   23719.000000  1.296050e+06   1956.634888  2.353383e+05      2.355642   \n",
      "std    13694.604047  1.694452e+06    145.802456  1.192507e+07      1.188805   \n",
      "min        0.000000  1.005000e+05      0.000000  0.000000e+00      0.000000   \n",
      "25%    11859.500000  5.650000e+05   1946.000000  4.991000e+03      2.000000   \n",
      "50%    23719.000000  9.600000e+05   1967.000000  6.502000e+03      2.000000   \n",
      "75%    35578.500000  1.525000e+06   1989.000000  1.045400e+04      3.000000   \n",
      "max    47438.000000  9.000000e+07   9999.000000  1.897474e+09     24.000000   \n",
      "\n",
      "       Full bathrooms  Total interior livable area  Total spaces  \\\n",
      "count    39574.000000                 4.491300e+04  46523.000000   \n",
      "mean         2.094961                 5.774587e+03      1.567117   \n",
      "std          0.963320                 8.324363e+05      9.011608   \n",
      "min          1.000000                 1.000000e+00    -15.000000   \n",
      "25%          2.000000                 1.187000e+03      0.000000   \n",
      "50%          2.000000                 1.566000e+03      1.000000   \n",
      "75%          2.000000                 2.142000e+03      2.000000   \n",
      "max         17.000000                 1.764164e+08   1000.000000   \n",
      "\n",
      "       Garage spaces  Elementary School Score  Elementary School Distance  \\\n",
      "count   46522.000000             42543.000000                42697.000000   \n",
      "mean        1.491746                 5.720824                    1.152411   \n",
      "std         8.964319                 2.103350                    2.332367   \n",
      "min       -15.000000                 1.000000                    0.000000   \n",
      "25%         0.000000                 4.000000                    0.300000   \n",
      "50%         1.000000                 6.000000                    0.500000   \n",
      "75%         2.000000                 7.000000                    1.000000   \n",
      "max      1000.000000                10.000000                   57.200000   \n",
      "\n",
      "       Middle School Score  Middle School Distance  High School Score  \\\n",
      "count         30734.000000            30735.000000       42220.000000   \n",
      "mean              5.317206                1.691593           6.134344   \n",
      "std               2.002768                2.462879           1.984711   \n",
      "min               1.000000                0.000000           1.000000   \n",
      "25%               4.000000                0.600000           5.000000   \n",
      "50%               5.000000                1.000000           6.000000   \n",
      "75%               7.000000                1.800000           8.000000   \n",
      "max               9.000000               57.200000          10.000000   \n",
      "\n",
      "       High School Distance  Tax assessed value  Annual tax amount  \\\n",
      "count          42438.000000        4.378700e+04       43129.000000   \n",
      "mean               2.410366        7.863118e+05        9956.843817   \n",
      "std                3.596120        1.157796e+06       13884.254976   \n",
      "min                0.000000        0.000000e+00           0.000000   \n",
      "25%                0.800000        2.549615e+05        3467.000000   \n",
      "50%                1.300000        5.475240e+05        7129.000000   \n",
      "75%                2.400000        9.371625e+05       12010.000000   \n",
      "max               73.900000        4.590000e+07      552485.000000   \n",
      "\n",
      "       Listed Price  Last Sold Price           Zip  \n",
      "count  4.743900e+04     2.967300e+04  47439.000000  \n",
      "mean   1.315890e+06     8.078537e+05  93279.178587  \n",
      "std    2.628695e+06     1.177903e+06   2263.459104  \n",
      "min    0.000000e+00     0.000000e+00  85611.000000  \n",
      "25%    5.745000e+05     3.350000e+05  90220.000000  \n",
      "50%    9.490000e+05     5.980000e+05  94114.000000  \n",
      "75%    1.498844e+06     9.500000e+05  95073.000000  \n",
      "max    4.025320e+08     9.000000e+07  96155.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', 'Bedrooms', 'Elementary School', 'Middle School', 'High School', 'Flooring', 'Heating features', 'Cooling features', 'Appliances included', 'Laundry features', 'Parking features']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 10000 to 6244 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
      "\t\t('int', [])                        :  1 | ['Zip']\n",
      "\t\t('object', [])                     :  4 | ['Type', 'Region', 'City', 'State']\n",
      "\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
      "\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
      "\t\t('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
      "\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
      "\t\t('int', [])                         :    1 | ['Zip']\n",
      "\t\t('int', ['binned', 'text_special']) :  181 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :    1 | ['State']\n",
      "\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
      "\t\t('int', ['text_ngram'])             : 5732 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in august', '__nlp__.000 in december', ...]\n",
      "\t56.3s = Fit runtime\n",
      "\t39 features in original data used to generate 5960 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 538.56 MB (3.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 57.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.05269925588650688, Train Rows: 44939, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 3.857 GB out of 16.439 GB available memory (23.461%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.22 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train KNeighborsUnif... Skipping this model.\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 3.857 GB out of 16.479 GB available memory (23.405%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.22 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train KNeighborsDist... Skipping this model.\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-351633.0862\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.3s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-320649.1736\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.23s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-336284.6763\t = Validation score   (-root_mean_squared_error)\n",
      "\t1650.74s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-409323.7804\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.04s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-308322.1701\t = Validation score   (-root_mean_squared_error)\n",
      "\t1833.91s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-417859.1992\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.51s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "c:\\Conda\\miniconda3\\envs\\automy\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:25:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Conda\\miniconda3\\envs\\automy\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:25:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-297142.4449\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.43s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-900186.0809\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.13s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-340850.7497\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.39s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMLarge': 0.36, 'XGBoost': 0.32, 'NeuralNetFastAI': 0.12, 'CatBoost': 0.08, 'ExtraTreesMSE': 0.08, 'NeuralNetTorch': 0.04}\n",
      "\t-253982.4213\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3713.72s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2046.9 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\DeepLearning\\AutogluonModels\\ag-20250130_022418\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "train_data = TabularDataset(r'C:\\DeepLearning\\california-house-prices\\train.csv')\n",
    "# train_data = pd.read_csv(download('kaggle_house_train'))\n",
    "#模型质量\n",
    "print(train_data.describe())\n",
    "id, label = 'Id', 'Sold Price'\n",
    "#数据预处理\n",
    "# larger_val_col = ['Lot', 'Total interior livable area',\n",
    "#                   'Tax assessed value','Annual tax amount',\n",
    "#                   'Listed Price', 'Last Sold Price']\n",
    "# for c in larger_val_col + [label]:\n",
    "#     train_data[c] = np.log(train_data[c]+1)\n",
    "\n",
    "    \n",
    "predictor = TabularPredictor(label=label).fit(\n",
    "    train_data.drop(columns=[id]),num_gpus=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: C:\\DeepLearning\\california-house-prices\\test.csv | Columns = 40 / 40 | Rows = 31626 -> 31626\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_data = TabularDataset(r'C:\\DeepLearning\\california-house-prices\\test.csv')\n",
    "predictions = predictor.predict(test_data.drop(columns=[id]))\n",
    "submission = pd.DataFrame({'Id': test_data[id], 'Sold Price': predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
